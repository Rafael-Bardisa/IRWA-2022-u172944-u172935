{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iiB2f3Y-5eXS",
   "metadata": {
    "id": "iiB2f3Y-5eXS"
   },
   "source": [
    "# Information Retrieval and Web Analytics\n",
    "\n",
    "# Part 3: Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
    "outputId": "ac3d643e-2de1-4fce-edc5-0556eb077b8e"
   },
   "outputs": [],
   "source": [
    "# mount google drive if using google collab, else skip\n",
    "# we are not using it because it is more comfortable to use jupyter lab\n",
    "\n",
    "BASEDIR = '.'\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASEDIR = 'drive/MyDrive'\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
    "outputId": "76dd31db-ad7c-4f5b-91f0-ffbc8c74a5b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rafaelbardisarodes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# required imports for the notebook\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from array import array\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import rank_bm25\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# queries for testing\n",
    "queries = [\n",
    "    \"keep us posted\",\n",
    "    \"ian update\",\n",
    "    \"disney world\",\n",
    "    \"climate change\",\n",
    "    \"hit state\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Iq8CBGFPEqTa",
   "metadata": {
    "id": "Iq8CBGFPEqTa"
   },
   "outputs": [],
   "source": [
    "# open results from last practice\n",
    "tweets = pd.read_csv(f'{BASEDIR}/data/processed_tweets.csv')\n",
    "tweets = tweets.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yPqSycI3REu6",
   "metadata": {
    "id": "yPqSycI3REu6"
   },
   "source": [
    "### Content from practice part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99eKgkLSGjsB",
   "metadata": {
    "id": "99eKgkLSGjsB"
   },
   "outputs": [],
   "source": [
    "# reuse of the function shown in class to transform text into lowercase and erase stop words in queries\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the line removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.lower()\n",
    "    \n",
    "    # tremendo pero aligual rompe algo despues\n",
    "    line = remove_punctuation(line)\n",
    "    \n",
    "    line = line.split()  # Tokenize the text to get a list of terms\n",
    "    line = [x for x in line if x not in stop_words]  # eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] # perform stemming (HINT: use List Comprehension)\n",
    "    return line\n",
    "\n",
    "\n",
    "@benchmark\n",
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "\n",
    "    for tweet in tweets.itertuples(index=True):  # Remember, lines contain all documents from file\n",
    "        tweet_text = tweet.full_text\n",
    "        \n",
    "        # tweet id\n",
    "        tweet_id = int(tweet.id.split(\"_\")[1])\n",
    "\n",
    "        terms = str(tweet_text).split(\" \")  # page_title + page_text\n",
    "\n",
    "        title_index[tweet_id] = tweet.user  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term] = [tweet_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "    return index, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Ib0gaZhiLvhD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ib0gaZhiLvhD",
    "outputId": "8dd8e3ea-877f-4577-ebda-f822d991cecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for \u001b[91mcreate_index\u001b[0m: 0.2113\n"
     ]
    }
   ],
   "source": [
    "index, title_index= create_index(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfVOqpgRLnc",
   "metadata": {
    "id": "nHfVOqpgRLnc"
   },
   "source": [
    "### 5 Text queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "F5aBt6wSRO_P",
   "metadata": {
    "id": "F5aBt6wSRO_P"
   },
   "outputs": [],
   "source": [
    "# def query(text)?:\n",
    "# build terms(query)\n",
    "\n",
    "# index = tremendo index\n",
    "# foreach term in query\n",
    "# index = index[term in entry] <- boolean mask, conjunctive AND\n",
    "\n",
    "# return index <- the doc ids\n",
    "# or\n",
    "# return tweets[original_text][index] <- get original text of tweets containing all elements in query\n",
    "\n",
    "def query(text, tweet_index=\"\"):\n",
    "    \"\"\"\n",
    "    search for a given text in the tweet collection using the\n",
    "    inverted index we previously computed\n",
    "    :param text: the query text\n",
    "    :param tweet_index: inverted index of the collection, named as such because context of practice\n",
    "    :return: list of tweet ids containing all (treated) terms in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    # necessary step since same treatment applied to tweets\n",
    "    terms = build_terms(text)\n",
    "    \n",
    "    # select tweet index, defaults to global index but can be specified\n",
    "    tweet_index = tweet_index if tweet_index else index\n",
    "    \n",
    "    plausible_ids = []\n",
    "    for query_term in terms:\n",
    "        # tweet_index[query_term] is list of tweet ids containing query term + position(s) in text, could be useful in the future\n",
    "        # plausible_ids[query_term] = tweet_index[query_term]\n",
    "        \n",
    "        # using sets is convenient for using reduce\n",
    "        plausible_ids.append(set(term_pos[0] for term_pos in tweet_index[query_term]))\n",
    "        \n",
    "    # reduce list of sets to intersection of all\n",
    "    relevant_ids = functools.reduce(lambda a, b: a.intersection(b), plausible_ids) if plausible_ids else []\n",
    "                             \n",
    "    return relevant_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f972cec-c2a7-4645-a28a-ab6a7225a533",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f972cec-c2a7-4645-a28a-ab6a7225a533",
    "outputId": "409d980f-fe89-45c3-94e5-fc9ae6b8b77b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_punctuation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [6], line 22\u001b[0m, in \u001b[0;36mquery\u001b[0;34m(text, tweet_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03msearch for a given text in the tweet collection using the\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03minverted index we previously computed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m:return: list of tweet ids containing all (treated) terms in the query\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# necessary step since same treatment applied to tweets\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m terms \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# select tweet index, defaults to global index but can be specified\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tweet_index \u001b[38;5;241m=\u001b[39m tweet_index \u001b[38;5;28;01mif\u001b[39;00m tweet_index \u001b[38;5;28;01melse\u001b[39;00m index\n",
      "Cell \u001b[0;32mIn [4], line 19\u001b[0m, in \u001b[0;36mbuild_terms\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m     16\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# tremendo pero aligual rompe algo despues\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[43mremove_punctuation\u001b[49m(line)\n\u001b[1;32m     21\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()  \u001b[38;5;66;03m# Tokenize the text to get a list of terms\u001b[39;00m\n\u001b[1;32m     22\u001b[0m line \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m line \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]  \u001b[38;5;66;03m# eliminate the stopwords\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_punctuation' is not defined"
     ]
    }
   ],
   "source": [
    "query(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bt_s1CwPRPmt",
   "metadata": {
    "id": "Bt_s1CwPRPmt"
   },
   "source": [
    "### Ranking results: TF-IDF + cosine score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M-z_0onURSJX",
   "metadata": {
    "id": "M-z_0onURSJX"
   },
   "outputs": [],
   "source": [
    "# rank(query):\n",
    "# query = build_terms(query)\n",
    "\n",
    "# scores = {}\n",
    "# length = {}\n",
    "# foreach term in query:\n",
    "# w_q = TF-IDF(term, query), docids = query(term)\n",
    "# foreach docid in docids:\n",
    "# scores[docid] += TF-IDF(term, tweets.full_text[docid]) * w_q\n",
    "# ::\n",
    "# foreach docid:\n",
    "# scores[d] /= len(tweets.full_text[docid])\n",
    "\n",
    "# sort scores, return top K\n",
    "\n",
    "\n",
    "# relevant documents = query(query)\n",
    "# foreach document in relevant_documents\n",
    "# TF-IDF(document, query)\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF(document, query):\n",
    "\n",
    "# len(query(term)) is df(term) if term is one word \n",
    "\n",
    "\n",
    "def tf_idf(term_freq, document_freq, collection_len):\n",
    "    if term_freq == 0 or document_freq == 0:\n",
    "        return 0\n",
    "    return (1 + math.log(term_freq)) * math.log(collection_len/document_freq)\n",
    "\n",
    "\n",
    "def doc_score(doc_id, collection_index=index, collection=\"\"):\n",
    "    \"\"\"\n",
    "    vector de scores para el documento dado, es lo que hay que usar para\n",
    "    la document length\n",
    "    \n",
    "    tremendo usarlo como {doc_id: doc_score(doc_id)} para todos los ids\n",
    "    :param doc_id: document id que mirar\n",
    "    :params: se supone que así será más flexible pero los defaults van finos asi que na\n",
    "    :return: diccionario de terms y pesos, util para normalizar documentos\n",
    "    \"\"\"\n",
    "    result={}\n",
    "    \n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    collection_len = len(collection)\n",
    "    \n",
    "    document = str(collection[doc_id]).split(\" \")\n",
    "    term_frequencies = Counter(document)\n",
    "    \n",
    "    for term in document:\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        result[term] = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "    return result\n",
    "\n",
    "\n",
    "@benchmark\n",
    "def collection_vectors(collection=\"\", collection_index=index):\n",
    "    \"\"\"\n",
    "    multi diccionario de documentos, terms y sus valores tf-idf\n",
    "    \"\"\"\n",
    "    document_vectors = {}\n",
    "    \n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    for doc_id, document in collection.items():\n",
    "        document_vectors[doc_id] = doc_score(doc_id, collection_index=collection_index, collection=collection)\n",
    "        \n",
    "    return document_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661e6fe-5da1-43fb-9c94-7bc5c39ac092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8661e6fe-5da1-43fb-9c94-7bc5c39ac092",
    "outputId": "99953f57-6ba7-4d68-fb6a-29cac880dbad"
   },
   "outputs": [],
   "source": [
    "document_lengths = collection_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864deed3-aaba-4ce4-a347-d1d97a3c2f6b",
   "metadata": {
    "id": "864deed3-aaba-4ce4-a347-d1d97a3c2f6b"
   },
   "outputs": [],
   "source": [
    "def cosine_score(query_text, collection_index=index, collection=\"\", lengths=document_lengths, k=10):\n",
    "    \"\"\"\n",
    "    computes cosine score of all documents in a collection against a query and ranks them\n",
    "    accordingly\n",
    "    \"\"\"\n",
    "\n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    collection_len = len(collection)\n",
    "           \n",
    "    scores = {doc_id: 0 for doc_id in collection.keys()}\n",
    "    \n",
    "    # esto seguramente este mal\n",
    "    # length = {doc_id: len(str(document).split(\" \")) for doc_id, document in collection.items()}\n",
    "    \n",
    "    query_terms = build_terms(query_text) # necessary step since same treatment applied to tweets\n",
    "    \n",
    "    # dictionary of frequency of each term in the query\n",
    "    query_frequencies = Counter(query_terms)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        # query of a term returns the set of documents containing the term\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        \n",
    "        query_weight = tf_idf(query_frequencies[term], document_freq, collection_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        for term in query_terms:\n",
    "        \n",
    "        # query of a term returns the set of documents containing the term\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        \n",
    "        query_weight = tf_idf(query_frequencies[term], document_freq, collection_len)\n",
    "        \n",
    "        # hasta aqui esta bien probablemente, despues pasa algo raro\n",
    "        for doc_id, document in collection.items():\n",
    "            # counter of distinct terms in document\n",
    "            term_frequencies = Counter(str(document).split(\" \"))\n",
    "            document_weight = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "            scores[doc_id] += query_weight * document_weight\n",
    "        \"\"\"\n",
    "            \n",
    "        for doc_id, document in collection.items():\n",
    "\n",
    "            term_frequencies = Counter(str(document).split(\" \"))\n",
    "            document_weight = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "\n",
    "            doc_vec = list(lengths[doc_id].values())\n",
    "            scores[doc_id] = query_weight * document_weight\n",
    "            \n",
    "    scores = {doc_id: score/np.linalg.norm(list(lengths[doc_id].values())) for doc_id, score in scores.items()}\n",
    "        \n",
    "    doc_ids_sorted = sorted(scores, key=scores.get, reverse=True)[:k]\n",
    "    return {doc_id: scores[doc_id] for doc_id in doc_ids_sorted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326befbc-2abc-42b3-9fe4-dc645ef5dfea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "326befbc-2abc-42b3-9fe4-dc645ef5dfea",
    "outputId": "ae56e5a4-96ac-4c49-8978-1210e88bb480"
   },
   "outputs": [],
   "source": [
    "cosine_score(\"keep us posted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253375e-7bc0-47b3-ad74-09841f8a38c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5253375e-7bc0-47b3-ad74-09841f8a38c6",
    "outputId": "4ced915e-813e-4e03-ae9a-c24f5c4014d9"
   },
   "outputs": [],
   "source": [
    "tweets[tweets['id'] == 'doc_3870']['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X2iI5vlIpVxT",
   "metadata": {
    "id": "X2iI5vlIpVxT"
   },
   "source": [
    "### Ranking results: our score + cosine similarity "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0F_YMqzipb8J",
   "metadata": {
    "id": "0F_YMqzipb8J"
   },
   "source": [
    "### Ranking results: BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3Jd9DRieqJMZ",
   "metadata": {
    "id": "3Jd9DRieqJMZ"
   },
   "outputs": [],
   "source": [
    "from bm25.rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "bm_collection = [build_terms(tweet.full_text) for tweet in tweets.itertuples(index=True)]\n",
    "\n",
    "bm25 = BM25Okapi(bm_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeA3-ZexrDC2",
   "metadata": {
    "id": "aeA3-ZexrDC2"
   },
   "source": [
    "### Top-20 list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ClLiBEVrrHGC",
   "metadata": {
    "id": "ClLiBEVrrHGC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
