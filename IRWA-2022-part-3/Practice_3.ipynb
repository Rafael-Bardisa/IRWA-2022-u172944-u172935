{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iiB2f3Y-5eXS",
   "metadata": {
    "id": "iiB2f3Y-5eXS"
   },
   "source": [
    "# Information Retrieval and Web Analytics\n",
    "\n",
    "# Part 2: Indexing and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
    "outputId": "ac3d643e-2de1-4fce-edc5-0556eb077b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# mount google drive if using google collab, else skip\n",
    "# we are not using it because it is more comfortable to use jupyter lab\n",
    "\n",
    "BASEDIR = '.'\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASEDIR = 'drive/MyDrive'\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
    "outputId": "76dd31db-ad7c-4f5b-91f0-ffbc8c74a5b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# required imports for the notebook\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "from array import array\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63017f6-7ced-4d7d-b4b9-875355f082ef",
   "metadata": {
    "id": "c63017f6-7ced-4d7d-b4b9-875355f082ef"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "RED = \"\\033[91m\"\n",
    "WHITE = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def benchmark(func):\n",
    "    \"\"\"\n",
    "    Decorador que te mide el tiempo que tarda la funcion en ejecutarse.\n",
    "    Se puede usar como cualquier funcion, e.g. benchmark(func),\n",
    "    pero al ser un decorador la gracia que tiene es que al hacer\n",
    "    @benchmark\n",
    "    def func():...\n",
    "    cada vez que uses func() estaras usando benchmark(func)()\n",
    "    :param func: la funcion que quieres testear\n",
    "    :return: la funcion original envuelta por el codigo de testeo\n",
    "    \"\"\"\n",
    "    def inner(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f'Time taken for {RED}{func.__name__}{WHITE}: {end - start:.4f}')\n",
    "        return result\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Iq8CBGFPEqTa",
   "metadata": {
    "id": "Iq8CBGFPEqTa"
   },
   "outputs": [],
   "source": [
    "# open results from last practice\n",
    "tweets = pd.read_csv(f'{BASEDIR}/data/processed_tweets.csv')\n",
    "tweets = tweets.reset_index()  # make sure indexes pair with number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yPqSycI3REu6",
   "metadata": {
    "id": "yPqSycI3REu6"
   },
   "source": [
    "### Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99eKgkLSGjsB",
   "metadata": {
    "id": "99eKgkLSGjsB"
   },
   "outputs": [],
   "source": [
    "# reuse of the function shown in class to transform text into lowercase and erase stop words in queries\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the line removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.lower()\n",
    "    \n",
    "    # tremendo pero aligual rompe algo despues\n",
    "    line = remove_punctuation(line)\n",
    "    \n",
    "    line = line.split()  # Tokenize the text to get a list of terms\n",
    "    line = [x for x in line if x not in stop_words]  # eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] # perform stemming (HINT: use List Comprehension)\n",
    "    return line\n",
    "\n",
    "\n",
    "@benchmark\n",
    "def create_index(tweets):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "\n",
    "    for tweet in tweets.itertuples(index=True):  # Remember, lines contain all documents from file\n",
    "        tweet_text = tweet.full_text\n",
    "        \n",
    "        # tweet id\n",
    "        tweet_id = int(tweet.id.split(\"_\")[1])\n",
    "\n",
    "        terms = str(tweet_text).split(\" \")  # page_title + page_text\n",
    "\n",
    "        title_index[tweet_id] = tweet.user  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term] = [tweet_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "    return index, title_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Ib0gaZhiLvhD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ib0gaZhiLvhD",
    "outputId": "8dd8e3ea-877f-4577-ebda-f822d991cecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for \u001b[91mcreate_index\u001b[0m: 0.1754\n"
     ]
    }
   ],
   "source": [
    "index, title_index= create_index(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nHfVOqpgRLnc",
   "metadata": {
    "id": "nHfVOqpgRLnc"
   },
   "source": [
    "### 5 Text queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "F5aBt6wSRO_P",
   "metadata": {
    "id": "F5aBt6wSRO_P"
   },
   "outputs": [],
   "source": [
    "# def query(text)?:\n",
    "# build terms(query)\n",
    "\n",
    "# index = tremendo index\n",
    "# foreach term in query\n",
    "# index = index[term in entry] <- boolean mask, conjunctive AND\n",
    "\n",
    "# return index <- the doc ids\n",
    "# or\n",
    "# return tweets[original_text][index] <- get original text of tweets containing all elements in query\n",
    "\n",
    "def query(text, tweet_index=\"\"):\n",
    "    \"\"\"\n",
    "    search for a given text in the tweet collection using the\n",
    "    inverted index we previously computed\n",
    "    :param text: the query text\n",
    "    :param tweet_index: inverted index of the collection, named as such because context of practice\n",
    "    :return: list of tweet ids containing all (treated) terms in the query\n",
    "    \"\"\"\n",
    "    \n",
    "    # necessary step since same treatment applied to tweets\n",
    "    terms = build_terms(text)\n",
    "    \n",
    "    # select tweet index, defaults to global index but can be specified\n",
    "    tweet_index = tweet_index if tweet_index else index\n",
    "    \n",
    "    plausible_ids = []\n",
    "    for query_term in terms:\n",
    "        # tweet_index[query_term] is list of tweet ids containing query term + position(s) in text, could be useful in the future\n",
    "        # plausible_ids[query_term] = tweet_index[query_term]\n",
    "        \n",
    "        # using sets is convenient for using reduce\n",
    "        plausible_ids.append(set(term_pos[0] for term_pos in tweet_index[query_term]))\n",
    "        \n",
    "    # reduce list of sets to intersection of all\n",
    "    relevant_ids = functools.reduce(lambda a, b: a.intersection(b), plausible_ids) if plausible_ids else []\n",
    "                             \n",
    "    return relevant_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f972cec-c2a7-4645-a28a-ab6a7225a533",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f972cec-c2a7-4645-a28a-ab6a7225a533",
    "outputId": "409d980f-fe89-45c3-94e5-fc9ae6b8b77b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bt_s1CwPRPmt",
   "metadata": {
    "id": "Bt_s1CwPRPmt"
   },
   "source": [
    "### Ranking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "M-z_0onURSJX",
   "metadata": {
    "id": "M-z_0onURSJX"
   },
   "outputs": [],
   "source": [
    "# rank(query):\n",
    "# query = build_terms(query)\n",
    "\n",
    "# scores = {}\n",
    "# length = {}\n",
    "# foreach term in query:\n",
    "# w_q = TF-IDF(term, query), docids = query(term)\n",
    "# foreach docid in docids:\n",
    "# scores[docid] += TF-IDF(term, tweets.full_text[docid]) * w_q\n",
    "# ::\n",
    "# foreach docid:\n",
    "# scores[d] /= len(tweets.full_text[docid])\n",
    "\n",
    "# sort scores, return top K\n",
    "\n",
    "\n",
    "# relevant documents = query(query)\n",
    "# foreach document in relevant_documents\n",
    "# TF-IDF(document, query)\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF(document, query):\n",
    "\n",
    "# len(query(term)) is df(term) if term is one word \n",
    "\n",
    "\n",
    "def tf_idf(term_freq, document_freq, collection_len):\n",
    "    if term_freq == 0 or document_freq == 0:\n",
    "        return 0\n",
    "    return (1 + math.log(term_freq)) * math.log(collection_len/document_freq)\n",
    "\n",
    "\n",
    "def doc_score(doc_id, collection_index=index, collection=\"\"):\n",
    "    \"\"\"\n",
    "    vector de scores para el documento dado, es lo que hay que usar para\n",
    "    la document length\n",
    "    \n",
    "    tremendo usarlo como {doc_id: doc_score(doc_id)} para todos los ids\n",
    "    :param doc_id: document id que mirar\n",
    "    :params: se supone que así será más flexible pero los defaults van finos asi que na\n",
    "    :return: diccionario de terms y pesos, util para normalizar documentos\n",
    "    \"\"\"\n",
    "    result={}\n",
    "    \n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    collection_len = len(collection)\n",
    "    \n",
    "    document = str(collection[doc_id]).split(\" \")\n",
    "    term_frequencies = Counter(document)\n",
    "    \n",
    "    for term in document:\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        result[term] = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "    return result\n",
    "\n",
    "\n",
    "@benchmark\n",
    "def collection_vectors(collection=\"\", collection_index=index):\n",
    "    \"\"\"\n",
    "    multi diccionario de documentos, terms y sus valores tf-idf\n",
    "    \"\"\"\n",
    "    document_vectors = {}\n",
    "    \n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    for doc_id, document in collection.items():\n",
    "        document_vectors[doc_id] = doc_score(doc_id, collection_index=collection_index, collection=collection)\n",
    "        \n",
    "    return document_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8661e6fe-5da1-43fb-9c94-7bc5c39ac092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8661e6fe-5da1-43fb-9c94-7bc5c39ac092",
    "outputId": "99953f57-6ba7-4d68-fb6a-29cac880dbad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for \u001b[91mcollection_vectors\u001b[0m: 22.9872\n"
     ]
    }
   ],
   "source": [
    "document_lengths = collection_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864deed3-aaba-4ce4-a347-d1d97a3c2f6b",
   "metadata": {
    "id": "864deed3-aaba-4ce4-a347-d1d97a3c2f6b"
   },
   "outputs": [],
   "source": [
    "def cosine_score(query_text, collection_index=index, collection=\"\", lengths=document_lengths, k=10):\n",
    "    \"\"\"\n",
    "    computes cosine score of all documents in a collection against a query and ranks them\n",
    "    accordingly\n",
    "    \"\"\"\n",
    "\n",
    "    collection = collection if collection else {tweet.id: tweet.full_text for tweet in tweets.itertuples(index=True)}\n",
    "    collection_len = len(collection)\n",
    "           \n",
    "    scores = {doc_id: 0 for doc_id in collection.keys()}\n",
    "    \n",
    "    # esto seguramente este mal\n",
    "    # length = {doc_id: len(str(document).split(\" \")) for doc_id, document in collection.items()}\n",
    "    \n",
    "    query_terms = build_terms(query_text) # necessary step since same treatment applied to tweets\n",
    "    \n",
    "    # dictionary of frequency of each term in the query\n",
    "    query_frequencies = Counter(query_terms)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        # query of a term returns the set of documents containing the term\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        \n",
    "        query_weight = tf_idf(query_frequencies[term], document_freq, collection_len)\n",
    "        \n",
    "        \"\"\"\n",
    "        for term in query_terms:\n",
    "        \n",
    "        # query of a term returns the set of documents containing the term\n",
    "        document_freq = len(query(term, tweet_index=collection_index))\n",
    "        \n",
    "        query_weight = tf_idf(query_frequencies[term], document_freq, collection_len)\n",
    "        \n",
    "        # hasta aqui esta bien probablemente, despues pasa algo raro\n",
    "        for doc_id, document in collection.items():\n",
    "            # counter of distinct terms in document\n",
    "            term_frequencies = Counter(str(document).split(\" \"))\n",
    "            document_weight = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "            scores[doc_id] += query_weight * document_weight\n",
    "        \"\"\"\n",
    "            \n",
    "        for doc_id, document in collection.items():\n",
    "\n",
    "            term_frequencies = Counter(str(document).split(\" \"))\n",
    "            document_weight = tf_idf(term_frequencies[term], document_freq, collection_len)\n",
    "\n",
    "            doc_vec = list(lengths[doc_id].values())\n",
    "            scores[doc_id] = query_weight * document_weight\n",
    "            \n",
    "    scores = {doc_id: score/np.linalg.norm(list(lengths[doc_id].values())) for doc_id, score in scores.items()}\n",
    "        \n",
    "    doc_ids_sorted = sorted(scores, key=scores.get, reverse=True)[:k]\n",
    "    return {doc_id: scores[doc_id] for doc_id in doc_ids_sorted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "326befbc-2abc-42b3-9fe4-dc645ef5dfea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "326befbc-2abc-42b3-9fe4-dc645ef5dfea",
    "outputId": "ae56e5a4-96ac-4c49-8978-1210e88bb480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_1546': 2.3995671065397546,\n",
       " 'doc_1069': 2.248496421335798,\n",
       " 'doc_1873': 1.9930095562822965,\n",
       " 'doc_2330': 1.642266340337091,\n",
       " 'doc_866': 1.4450083632022115,\n",
       " 'doc_2640': 1.3283712106499805,\n",
       " 'doc_1390': 1.2418825485022358,\n",
       " 'doc_91': 1.1303636373681631,\n",
       " 'doc_3870': 1.0782249055799842,\n",
       " 'doc_3923': 1.069995714837875}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_score(\"keep us posted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5253375e-7bc0-47b3-ad74-09841f8a38c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5253375e-7bc0-47b3-ad74-09841f8a38c6",
    "outputId": "4ced915e-813e-4e03-ae9a-c24f5c4014d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3869    sorri post much storm. hope understand. here’ ...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['id'] == 'doc_3870']['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zcniTIFwRStU",
   "metadata": {
    "id": "zcniTIFwRStU"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4hhTKjnnSOfA",
   "metadata": {
    "id": "4hhTKjnnSOfA"
   },
   "outputs": [],
   "source": [
    "# useful functions for the evaluation\n",
    "def precision_k (y_true, y_score, k=10, val_order='up'):\n",
    "    val = 1 if val_order == \"up\" else -1\n",
    "    order = y_score.argsort()[::val]\n",
    "    y_true = y_true.take(order)\n",
    "    relevant = np.sum(y_true[:k])\n",
    "    return float(relevant/k)\n",
    "\n",
    "def recall_k (y_true, y_score, k=10, val_order='up'):\n",
    "    val = 1 if val_order == \"up\" else -1\n",
    "    order = y_score.argsort()[::val]\n",
    "    y_true = y_true.take(order)\n",
    "    relevant = np.sum(y_true[:k])\n",
    "    total_relevant = np.sum(y_true[:])\n",
    "    return float(relevant/total_relevant)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    return (2*precision*recall)/(precision+recall)\n",
    "\n",
    "def avg_precision_at_k(doc_score, y_score, k=10, val_order='up'):\n",
    "    val = 1 if val_order == \"up\" else -1\n",
    "    gtp = np.sum(doc_score == 1)\n",
    "    order = np.argsort(y_score)[::val]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1:\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i + 1)\n",
    "    return prec_at_i / gtp\n",
    "\n",
    "def map_at_k(search_res, k=10):\n",
    "    avp = []\n",
    "    for q in search_res[\"query_id\"].unique():\n",
    "        curr_data = search_res[search_res[\"query_id\"] == q]\n",
    "        avp.append(avg_precision_at_k(np.array(curr_data[\"label\"]), \n",
    "                   np.array(curr_data[\"doc\"]), k))\n",
    "    return np.sum(avp) / len(avp)\n",
    "\n",
    "def rr_at_k(doc_score, y_score, k=10, val_order='up'):\n",
    "    val = 1 if val_order == \"up\" else -1\n",
    "    order = np.argsort(y_score)[::val]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    if np.sum(doc_score) == 0:\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)\n",
    "\n",
    "def dcg_at_k(doc_score, y_score, k=10, val_order='up'):\n",
    "    val = -1 if val_order == \"up\" else 1\n",
    "    order = np.argsort(y_score)[::val]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    gain = 2 ** doc_score - 1\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_at_k(doc_score, y_score, k=10, val_order='up'):\n",
    "    dcg_max = dcg_at_k(doc_score, doc_score, k, val_order=val_order)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vvo6nCpRRx7m",
   "metadata": {
    "id": "vvo6nCpRRx7m"
   },
   "source": [
    "#### Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "k-VfnWbARavi",
   "metadata": {
    "id": "k-VfnWbARavi"
   },
   "outputs": [],
   "source": [
    "# read the new csv file as a dataframe\n",
    "with open(f'{BASEDIR}/data/evaluation_gt.csv', 'r') as file:\n",
    "    ev_array = file.readlines()\n",
    "    ev_array = [row.rstrip().split(',') for row in ev_array]\n",
    "df_evaluation = pd.DataFrame(ev_array[1:], \n",
    "             columns=ev_array[0])\n",
    "for index, row in df_evaluation.iterrows():\n",
    "  df_evaluation['doc'][index] = int(df_evaluation['doc'][index].split(\"_\")[1])\n",
    "df_evaluation['query_id'] = pd.to_numeric(df_evaluation['query_id'])\n",
    "df_evaluation['label'] = pd.to_numeric(df_evaluation['label'])\n",
    "#df = df.set_index('doc').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "VzC5IJp5VAL3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzC5IJp5VAL3",
    "outputId": "c4d8b91c-350f-4539-93ef-7d48adc2fe0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "==> Precision@10: 0.9\n",
      "\n",
      "==> Recall@10: 0.9\n",
      "\n",
      "==> F1score@10: 0.9\n",
      "\n",
      "==> AveragePrecision@10: 0.89\n",
      "\n",
      "==> MAP@10: 0.777420634920635\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0.0734\n",
      "\n",
      "Query 2\n",
      "==> Precision@10: 0.8\n",
      "\n",
      "==> Recall@10: 0.8\n",
      "\n",
      "==> F1score@10: 0.8000000000000002\n",
      "\n",
      "==> AveragePrecision@10: 0.5758730158730159\n",
      "\n",
      "==> MAP@10: 0.777420634920635\n",
      "\n",
      "==> RR@10: 0.5\n",
      "\n",
      "==> NDCG@10: 0.1396\n",
      "\n",
      "Query 3\n",
      "==> Precision@10: 0.9\n",
      "\n",
      "==> Recall@10: 0.9\n",
      "\n",
      "==> F1score@10: 0.9\n",
      "\n",
      "==> AveragePrecision@10: 0.866388888888889\n",
      "\n",
      "==> MAP@10: 0.777420634920635\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0.0663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in [1,2,3]:\n",
    "  current_query_res = df_evaluation[df_evaluation[\"query_id\"] == q]\n",
    "  k = 10\n",
    "  precision = precision_k(current_query_res[\"label\"], current_query_res[\"doc\"], k, val_order='up')\n",
    "  recall = recall_k(current_query_res[\"label\"], current_query_res[\"doc\"], k, val_order='up')\n",
    "  print(\"Query {}\".format(q))\n",
    "  print(\"==> Precision@{}: {}\\n\".format(k, precision))\n",
    "  print(\"==> Recall@{}: {}\\n\".format(k, recall))\n",
    "  print(\"==> F1score@{}: {}\\n\".format(k, f1_score(precision, recall)))\n",
    "  print(\"==> AveragePrecision@{}: {}\\n\".format(k, avg_precision_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"doc\"]), k, val_order='up')))\n",
    "  print(\"==> MAP@{}: {}\\n\".format(k, map_at_k(df_evaluation, k)))\n",
    "  print(\"==> RR@{}: {}\\n\".format(k, rr_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"doc\"]), k, val_order='up')))\n",
    "  print(\"==> NDCG@{}: {}\\n\".format(k, ndcg_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"doc\"]), k, val_order='up')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AH_dcoduR7C7",
   "metadata": {
    "id": "AH_dcoduR7C7"
   },
   "source": [
    "#### Defined queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "la13ygcWhflV",
   "metadata": {
    "id": "la13ygcWhflV"
   },
   "outputs": [],
   "source": [
    "# read the new csv file as a dataframe\n",
    "with open(f'{BASEDIR}/data/evaluation_tweets.csv', 'r') as file:\n",
    "    ev_array = file.readlines()\n",
    "    ev_array = [row.rstrip().split(',') for row in ev_array]\n",
    "df_queries = pd.DataFrame(ev_array[1:], \n",
    "             columns=ev_array[0])\n",
    "for index, row in df_queries.iterrows():\n",
    "    df_queries['doc'][index] = int(df_queries['doc'][index].split(\"_\")[1])\n",
    "df_queries['query_id'] = pd.to_numeric(df_queries['query_id'])\n",
    "df_queries['label'] = pd.to_numeric(df_queries['label'])\n",
    "\n",
    "queries = [\"keep us posted\", \"ian update\",\"disney world\", \"climate change\", \"hit state\"]\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(1,4001):\n",
    "        if ((df_queries['query_id'] == i+1) & (df_queries['doc'] == j)).any() == False:\n",
    "            new_row = {'doc': j, 'query_id': i+1, 'label': 0, 'predicted': 0.0}\n",
    "            df_queries = df_queries.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dew4P_8AeNNZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dew4P_8AeNNZ",
    "outputId": "9c5790bb-6692-4e6f-8715-8f0ba10bee79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# add predicted values\n",
    "df_queries['predicted'] = pd.to_numeric(df_queries['predicted'])\n",
    "for i in range(5):\n",
    "  cosine = cosine_score(queries[i], k=50)\n",
    "  a = [int(key.split(\"_\")[1]) for key in cosine.keys()]\n",
    "  for val in a:\n",
    "    index = df_queries.index[(df_queries['doc'] == val) & (df_queries['query_id'] == i+1)].to_list()\n",
    "    df_queries['predicted'][index[0]] = cosine['doc_'+str(val)]\n",
    "df_queries = df_queries.fillna({'predicted': 0.0})\n",
    "df_queries['label'] = df_queries['predicted'].apply(lambda y: 1 if y >= 1.2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14grXE0dXYyd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14grXE0dXYyd",
    "outputId": "98060872-eeb3-468b-82ba-f35e2ac8696e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1\n",
      "==> Precision@10: 0.7\n",
      "\n",
      "==> Recall@10: 1.0\n",
      "\n",
      "==> F1score@10: 0.8235294117647058\n",
      "\n",
      "==> AveragePrecision@10: 1.0\n",
      "\n",
      "==> MAP@10: 0.0\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0\n",
      "\n",
      "Query 2\n",
      "==> Precision@10: 0.7\n",
      "\n",
      "==> Recall@10: 1.0\n",
      "\n",
      "==> F1score@10: 0.8235294117647058\n",
      "\n",
      "==> AveragePrecision@10: 1.0\n",
      "\n",
      "==> MAP@10: 0.0\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0\n",
      "\n",
      "Query 3\n",
      "==> Precision@10: 0.7\n",
      "\n",
      "==> Recall@10: 1.0\n",
      "\n",
      "==> F1score@10: 0.8235294117647058\n",
      "\n",
      "==> AveragePrecision@10: 1.0\n",
      "\n",
      "==> MAP@10: 0.0\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0\n",
      "\n",
      "Query 4\n",
      "==> Precision@10: 0.8\n",
      "\n",
      "==> Recall@10: 1.0\n",
      "\n",
      "==> F1score@10: 0.888888888888889\n",
      "\n",
      "==> AveragePrecision@10: 1.0\n",
      "\n",
      "==> MAP@10: 0.0\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0\n",
      "\n",
      "Query 5\n",
      "==> Precision@10: 0.4\n",
      "\n",
      "==> Recall@10: 1.0\n",
      "\n",
      "==> F1score@10: 0.5714285714285715\n",
      "\n",
      "==> AveragePrecision@10: 1.0\n",
      "\n",
      "==> MAP@10: 0.0\n",
      "\n",
      "==> RR@10: 1.0\n",
      "\n",
      "==> NDCG@10: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in [1,2,3,4,5]:\n",
    "  current_query_res = df_queries[df_queries[\"query_id\"] == q]\n",
    "  k = 10\n",
    "  precision = precision_k(current_query_res[\"label\"], current_query_res[\"predicted\"], k, val_order='down')\n",
    "  recall = recall_k(current_query_res[\"label\"], current_query_res[\"predicted\"], k, val_order='down')\n",
    "  print(\"Query {}\".format(q))\n",
    "  print(\"==> Precision@{}: {}\\n\".format(k, precision))\n",
    "  print(\"==> Recall@{}: {}\\n\".format(k, recall))\n",
    "  print(\"==> F1score@{}: {}\\n\".format(k, f1_score(precision, recall)))\n",
    "  print(\"==> AveragePrecision@{}: {}\\n\".format(k, avg_precision_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"predicted\"]), k, val_order='down')))\n",
    "  print(\"==> MAP@{}: {}\\n\".format(k, map_at_k(df_queries, k)))\n",
    "  print(\"==> RR@{}: {}\\n\".format(k, rr_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"predicted\"]), k, val_order='down')))\n",
    "  print(\"==> NDCG@{}: {}\\n\".format(k, ndcg_at_k(np.array(current_query_res[\"label\"]), np.array(current_query_res[\"predicted\"]), k, val_order='down')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_VH9QM2xSCIt",
   "metadata": {
    "id": "_VH9QM2xSCIt"
   },
   "source": [
    "### Tweet representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "EFv7uwjzSHqk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "EFv7uwjzSHqk",
    "outputId": "5be3a981-525c-4bd0-b781-2e68a1fc126f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clean_tweets = []\n",
    "for tweet in tweets.itertuples(index=True): \n",
    "        clean_tweets.append(str(tweet.full_text))\n",
    "model = Word2Vec(clean_tweets, workers=4, min_count=50, window=10, sample=1e-3)\n",
    "\n",
    "X = model.wv[model.wv.key_to_index]\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
