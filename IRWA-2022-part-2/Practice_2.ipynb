{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "iiB2f3Y-5eXS",
      "metadata": {
        "id": "iiB2f3Y-5eXS"
      },
      "source": [
        "# Information Retrieval and Web Analytics\n",
        "\n",
        "# Part 2: Indexing and evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
        "outputId": "302ed6ae-5372-44b3-eec9-5e83a854e875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive if using google collab, else skip\n",
        "# we are not using it because it is more comfortable to use jupyter lab\n",
        "\n",
        "BASEDIR = '.'\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASEDIR = 'drive/MyDrive'\n",
        "    \n",
        "except ModuleNotFoundError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
      "metadata": {
        "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
        "outputId": "b6544122-72ef-4d0b-cecf-831928a25df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# required imports for the notebook\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "from array import array\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# open results from last practice\n",
        "tweets = pd.read_csv(f'{BASEDIR}/data/processed_tweets.csv')\n",
        "tweets = tweets.reset_index()  # make sure indexes pair with number of rows\n",
        "\n",
        "# read the new csv file as a dataframe\n",
        "with open(f'{BASEDIR}/data/evaluation_gt.csv', 'r') as file:\n",
        "    ev_array = file.readlines()\n",
        "    ev_array = [row.rstrip().split(',') for row in ev_array]\n",
        "df = pd.DataFrame(ev_array[1:], \n",
        "             columns=[ev_array[0]])"
      ],
      "metadata": {
        "id": "Iq8CBGFPEqTa"
      },
      "id": "Iq8CBGFPEqTa",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create inverted index\n",
        "def create_index(tweets):\n",
        "    \"\"\"\n",
        "    Impleent the inverted index\n",
        "    \n",
        "    Argument:\n",
        "    collection of tweets\n",
        "    \n",
        "    Returns:\n",
        "    index - the inverted index containing terms as keys and the corresponding \n",
        "    list of tweets these keys appears in (and the positions) as values.\n",
        "    tf - normalized term frequency for each term in each tweet\n",
        "    idf - inverse document frequency of each term\n",
        "    \"\"\"\n",
        "    \n",
        "    index = [] \n",
        "    \n",
        "    tf = []         # term frequencies of terms in tweets \n",
        "    df = []         # tweet frequencies of terms in the document collection\n",
        "    idf = []\n",
        "    \n",
        "    N = len(tweets)\n",
        "    \n",
        "    for index, tweet in tweets.iterrows(): \n",
        "        tweet_id = tweet['id']        # get the id of the tweet\n",
        "        terms = tweet['full_text']    # get the tweet content\n",
        "                \n",
        "        termdictTweet = {}\n",
        "\n",
        "        for position, term in enumerate(terms): # terms in the tweet\n",
        "            try:\n",
        "                # if the term is already in the index for the current tweet\n",
        "                # append the position to the corrisponding list\n",
        "                termdictTweet[term][tweet_id].append(position)  \n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                termdictTweet[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
        "        \n",
        "        \n",
        "        # normalize term frequencies\n",
        "        # Compute the denominator to normalize term frequencies\n",
        "        # norm is the same for all terms of a tweet.\n",
        "        norm = 0\n",
        "        for term, posting in termdictTweet.items(): \n",
        "            # posting is a list containing tweet_id and the list of positions for current term in current tweet: \n",
        "            # posting ==> [tweet_id, [list of positions]] \n",
        "            # you can use it to inferr the frequency of current term.\n",
        "            norm += len(posting[1])**2\n",
        "        \n",
        "        norm = math.sqrt(norm)\n",
        "\n",
        "        # calculate the tf (dividing the term frequency by the above computed norm) and df weights\n",
        "        for term, posting in termdictTweet.items():     \n",
        "            # append the tf for current term (tf = term frequency in current tweet/norm)\n",
        "            tf[term].append(np.round(len(posting[1])/norm ,4))  \n",
        "            # increment the document frequency of current term (number of tweets containing the current term)\n",
        "            df[term] += 1  # increment df for current term\n",
        "        \n",
        "        # Compute idf \n",
        "        for term in df:\n",
        "            idf[term] = np.round(np.log(float(N/df[term])),4)\n",
        "        \n",
        "        # merge the current tweet index with the main index\n",
        "        for termpage, postingpage in termdictTweet.items():\n",
        "            index[termpage].append(postingpage)\n",
        "                      \n",
        "                    \n",
        "    return index, tf, idf\n"
      ],
      "metadata": {
        "id": "99eKgkLSGjsB"
      },
      "id": "99eKgkLSGjsB",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverted_index, tf, idf= create_index(tweets)"
      ],
      "metadata": {
        "id": "Ib0gaZhiLvhD",
        "outputId": "0d3178a9-fa67-4e9b-9d56-5e5d92ee6a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "id": "Ib0gaZhiLvhD",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-55dd4f35a9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minverted_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-00c95eaf6d41>\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m         \u001b[0;31m# term frequencies of terms in tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m         \u001b[0;31m# tweet frequencies of terms in the document collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gi-pKx0oMj4B"
      },
      "id": "Gi-pKx0oMj4B",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}