{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiB2f3Y-5eXS"
      },
      "source": [
        "# Information Retrieval and Web Analytics\n",
        "\n",
        "# Indexing + Modeling (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVfEcBBOZUbK",
        "outputId": "51c832fb-cd20-4ac2-ae3d-78180e6589ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tux35kpU5eXX"
      },
      "source": [
        "Welcome to the first hands-on session of Information Retrieval and Web Analytics!\n",
        "\n",
        "In this exercise you will implement a simple search engine to query a sample of Wikipedia articles. You will be provided with a sample of 500 Wikipedia articles in text format (some preprocessing has already been done to remove html tags).\n",
        "\n",
        "For each article you have the following features:\n",
        "\n",
        "- article id\n",
        "- article title\n",
        "- article body\n",
        "\n",
        "This session is composed by three main parts:\n",
        "\n",
        "1. **Create the index by going through the documents**\n",
        "2. **Query the index to obtain a set of documents**\n",
        "3. **Add some ranking to obtain a sorted set of documents when querying the index**\n",
        "\n",
        "\n",
        "## 1. Create the index\n",
        "The index is implemented through an **Inverted Index** which is the main data structure of our search engine. It maps the terms of our corpus (the collection of documents) to the documents that those terms appear in.\n",
        "\n",
        "You will implement the index through a Python dictionary, and then you will use it to return the list of documents relevant for a query.\n",
        "\n",
        "Each **vocabulary term** is a key in the index whose value is the list of documents that the term appears in.\n",
        "![figure 1](https://drive.google.com/uc?export=view&id=1IevTrOCl6vfupXYNnBBK0kQ7LRegNwyO)\n",
        "\n",
        "<caption><center> <u> <font color=''> Figure 1 </u><font color=''>  : Example of an inverted index</center></caption>\n",
        "    \n",
        "*Figure 1* shows a basic implementation of an inverted index. However, there exists a special type of queries, named **Phrase Queries**, where the position of the terms in the document matters. Phrase Queries are those queries typed inside double quotes when we want the matching documents to contain the terms in the query exactly in the specified order.\n",
        "    \n",
        "In order to work with Phrase Queries we need to add some information in the inverted index. The new inverted index will store, for each term, the list of documents containing the term and the positions of the term in the corresponding document.\n",
        "\n",
        "See *Figure 2*: \n",
        "    \n",
        "![figure 2](https://drive.google.com/uc?export=view&id=1Le9fW_spJRNNWgTliHK6vAcMNAOJDJY8)\n",
        "\n",
        "<caption><center> <u> <font color=''> Figure 2 </u><font color=''>  : Example of an inverted index with term's position information </center></caption>\n",
        "\n",
        "\n",
        "In the above example the term *Information* appears in document 1 at positions 0 (we start counting positions from 0), and in document 3 at position 0.\n",
        "    \n",
        "Notice that when implementing the index, you will need to perform some preprocessing:\n",
        "    \n",
        "    - Transform all words to lower case ( we donâ€™t want to index *Information*, *information*, and *INFORMATION* differently.)\n",
        "    - Remove stop words ( very common words like articles, etc.)\n",
        "    - Apply Stemming (remove common endings from words. For example the stemmed version of the words fish, fishes, fishing, fisher, fished is the word 'fish')\n",
        "    \n",
        "But do not worry about that, we will provide you with simple tools to do it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yioorw85eXZ"
      },
      "source": [
        "### Index implementation\n",
        "To create the index you will perform the following steps:\n",
        "- Loop over all documents of the collection provided in the dataset found in the project file `inputs/documents-corpus.tsv`.\n",
        "- Concatenate the title and the text of the page.\n",
        "- Lowercase all words.\n",
        "- Get tokens (transform the string title+body into a list of terms)\n",
        "- Remove stop words\n",
        "- Stem each token\n",
        "- Build the index following the model of Figure 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNt6V-hu5eXa"
      },
      "source": [
        "#### Load Python packages\n",
        "Let's first import all the packages that you will need during this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Z27uXzYa5eXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920dac8f-7777-48bd-ab73-cb3ea95b4dde"
      },
      "source": [
        "# if you do not have 'nltk', the following command should work \"python -m pip install nltk\"\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pQ4wsl7O5eXc"
      },
      "source": [
        "from collections import defaultdict\n",
        "from array import array\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "import numpy as np\n",
        "import collections\n",
        "from numpy import linalg as la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdOqU3nR5eXc"
      },
      "source": [
        "#### Load data into memory\n",
        "The dataset is stored in the TSV file, and it contains 500 Wikipedia articles (one article per line). For each article we have the document id, document title and document body separated by \"|\" character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a5z65YP5eXd"
      },
      "source": [
        "docs_path = 'your_path/inputs/documents-corpus.tsv'\n",
        "with open(docs_path) as fp:\n",
        "    lines = fp.readlines()\n",
        "lines = [l.strip().replace(' +', ' ') for l in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1urImOuK5eXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4835b791-c872-48da-e14c-21563f37b332"
      },
      "source": [
        "print(\"Total number of Wikipedia articles in the corpus: {}\".format(len(lines)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of Wikipedia articles in the corpus: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c21sBGa5eXe"
      },
      "source": [
        "Implement the function ```get_terms(line)```.\n",
        "\n",
        "It takes as input a text and performs the following operations:\n",
        "\n",
        "- Transform all text to lowercase\n",
        "- Tokenize the text to get a list of terms (use *split function*)\n",
        "- Remove stop words\n",
        "- Stem terms (example: to stem the term 'researcher', you will use ```stemming.stem(researcher)```)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeYKSVnH5eXf"
      },
      "source": [
        "def build_terms(line):\n",
        "    \"\"\"\n",
        "    Preprocess the article text (title + body) removing stop words, stemming,\n",
        "    transforming in lowercase and return the tokens of the text.\n",
        "    \n",
        "    Argument:\n",
        "    line -- string (text) to be preprocessed\n",
        "    \n",
        "    Returns:\n",
        "    line - a list of tokens corresponding to the input text after the preprocessing\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    line = line.lower()\n",
        "    line = line.split()  # Tokenize the text to get a list of terms\n",
        "    line = [x for x in line if x not in stop_words]  # eliminate the stopwords\n",
        "    line = [stemmer.stem(word) for word in line] # perform stemming (HINT: use List Comprehension)\n",
        "    return line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gvXQ7Vg5eXf"
      },
      "source": [
        "def create_index(lines):\n",
        "    \"\"\"\n",
        "    Implement the inverted index\n",
        "    \n",
        "    Argument:\n",
        "    lines -- collection of Wikipedia articles\n",
        "    \n",
        "    Returns:\n",
        "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
        "    list of documents where these keys appears in (and the positions) as values.\n",
        "    \"\"\"\n",
        "    index = defaultdict(list)\n",
        "    title_index = {}  # dictionary to map page titles to page ids\n",
        "\n",
        "    for line in lines:  # Remember, lines contain all documents from file\n",
        "        line_arr = line.split(\"|\")\n",
        "        page_id = int(line_arr[0])\n",
        "\n",
        "        terms = build_terms(''.join(line_arr[1:]))  # page_title + page_text\n",
        "\n",
        "        title = line_arr[1]\n",
        "        title_index[page_id] = title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
        "        \n",
        "        ## ===============================================================        \n",
        "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
        "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
        "\n",
        "        ## Example: if the curr_doc has id 1 and his text is \"web retrieval information retrieval\":\n",
        "\n",
        "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
        "\n",
        "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
        "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
        "        ## ===============================================================\n",
        "\n",
        "        current_page_index = {}\n",
        "\n",
        "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
        "            try:\n",
        "                # if the term is already in the index for the current page (current_page_index)\n",
        "                # append the position to the corresponding list\n",
        "                current_page_index[term][1].append(position)\n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
        "\n",
        "        # merge the current page index with the main index\n",
        "        for term_page, posting_page in current_page_index.items():\n",
        "            index[term_page].append(posting_page)\n",
        "    return index, title_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BawBFy3a5eXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e1c14a-5809-4aff-a95c-59302077d930"
      },
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "index, title_index = create_index(lines)\n",
        "print(\"Total time to create the index: {} seconds\".format(np.round(time.time() - start_time, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time to create the index: 8.89 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiJKluh5eXg"
      },
      "source": [
        "Notice that if you look in the index for ```researcher```you will not find any result, while if you look for ```research``` you will get some results. That happens because we are storing in the index stemmed terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BIjIMayR5eXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb69fb2-acb2-4483-9f63-261e18760831"
      },
      "source": [
        "print(\"Index results for the term 'researcher': {}\\n\".format(index['researcher']))\n",
        "print(\"First 10 Index results for the term 'research': \\n{}\".format(index['research'][:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index results for the term 'researcher': []\n",
            "\n",
            "First 10 Index results for the term 'research': \n",
            "[[33, array('I', [76])], [104, array('I', [633])], [131, array('I', [1257])], [139, array('I', [84])], [183, array('I', [979, 1111, 1302, 1411])], [189, array('I', [335, 362])], [203, array('I', [269])], [214, array('I', [44])], [268, array('I', [659])], [289, array('I', [623, 751, 774])]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zidHh2Jl5eXh"
      },
      "source": [
        "## 2. Querying the Index\n",
        "\n",
        "Even if before we mentioned that in case of phrase queries we need to take into account the position of the terms in the document and we have implemented an index that would allow us to also work with this type of queries, here you are going to implement a search function that will query the index without take into account the terms' positions.\n",
        "\n",
        "\n",
        "We will use english Free Text Queries, that means that the query we will query the index using  a sequence of english words as query, and the output will be the list of documents that contain any of the query terms. \n",
        "\n",
        "For instance if we write the query **\"computer science\"** the output will be the union of all documents containing the term \"computer\" with all documents containing the term \"science\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHbxu985eXi"
      },
      "source": [
        "def search(query, index):\n",
        "    \"\"\"\n",
        "    The output is the list of documents that contain any of the query terms. \n",
        "    So, we will get the list of documents for each query term, and take the union of them.\n",
        "    \"\"\"\n",
        "    query = build_terms(query)\n",
        "    docs = set()\n",
        "    for term in query:\n",
        "        try:\n",
        "            # store in term_docs the ids of the docs that contain \"term\"\n",
        "            term_docs = [posting[0] for posting in index[term]]\n",
        "            # docs = docs Union term_docs\n",
        "            docs |= set(term_docs)\n",
        "        except:\n",
        "            #term is not in index\n",
        "            pass\n",
        "    docs = list(docs)\n",
        "    return docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORYODejF5eXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162cd96b-d03a-4426-b265-c69128011fa3"
      },
      "source": [
        "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
        "query = input()\n",
        "docs = search(query, index)\n",
        "top = 10\n",
        "\n",
        "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
        "for d_id in docs[:top]:\n",
        "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert your query (i.e.: Computer Science):\n",
            "\n",
            "Computer Science\n",
            "\n",
            "======================\n",
            "Sample of 10 results out of 345 for the searched query:\n",
            "\n",
            "page_id= 1029 - page_title: Adjoint state method\n",
            "page_id= 2059 - page_title: Apache Cassandra\n",
            "page_id= 1036 - page_title: Adminer\n",
            "page_id= 3089 - page_title: BCSWomen\n",
            "page_id= 1043 - page_title: Admissible heuristic\n",
            "page_id= 1046 - page_title: Admon\n",
            "page_id= 26 - page_title: 12th Computer Olympiad\n",
            "page_id= 3103 - page_title: BESM\n",
            "page_id= 33 - page_title: 18 bit\n",
            "page_id= 1059 - page_title: Adobe Flash\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-KN2OIM5eXi"
      },
      "source": [
        "## 3. Add Ranking with TF-IDF\n",
        "\n",
        "When searching in a search engine, we are interested in obtain the results sorted by relevance or by some other criteria. Notice that **the above results are not ranked**.\n",
        "\n",
        "Here you are going to implement the **TF-IDF (Term Frequency â€” Inverse Document Frequency)** mechanism and use it to obtain a list of ordered results.\n",
        "\n",
        "TF-IDF is a weighting scheme that assigns each term in a document a weight based on its term frequency (FT) and the inverse document frequency (IDF).  The higher the scores, more important the term is. \n",
        "\n",
        "##### TF\n",
        "**TF** refers to the frequency of a term $t$ in a specific document $d$. The basic idea is that as a term appears more in the document it becomes more important. On the other side, if we only use pure term counts, longer documents will be favored more. Consider two documents with exactly the same content but one being twice longer by concatenating with itself.  The tf weights of each word in the longer document will be twice the shorter one, although they essentially have the same content. To deal with this issue we need to **normalize the term frequencies**.\n",
        "\n",
        "\n",
        "$$tf_{t,d} = \\dfrac{N_{t,d}}{||D||}\\tag{1}$$\n",
        "\n",
        "\n",
        "where ||D|| is the Euclidean norm. \n",
        "\n",
        "\n",
        "Let $D=[t_1, t_2, \\dots, t_n]$ be the document vector where $t_i$ represent the frequency of the term $i$, the  Euclidean Norm is calculated as\n",
        "\n",
        "\n",
        "$$\\sqrt{\\sum_{t=1}^{n}t_i{^2}}\\tag{2}$$\n",
        "\n",
        "\n",
        "Note that $||D||$ is the same for all terms of a document.\n",
        "\n",
        "\n",
        "##### IDF\n",
        "A drawback of tf is that it considers all terms equally important. However, less common terms are more discriminative than others. To deal with this issue we introduce **idf (inverse document frequency)** that takes into account the number of documents containing the term.\n",
        "\n",
        "$$idf_t = log\\dfrac{N}{df_t}\\tag{3}$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "- $N$ is the total number of documents;\n",
        "- $df_t$ is the number of documents containing the term $t$.\n",
        "\n",
        "The log operation is applied to avoid that terms that appears in a high number of documents are considered to be too much less important, in this way we are smoothing (dampening) this difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPpdtRY5eXi"
      },
      "source": [
        "def create_index_tfidf(lines, num_documents):\n",
        "    \"\"\"\n",
        "    Implement the inverted index and compute tf, df and idf\n",
        "    \n",
        "    Argument:\n",
        "    lines -- collection of Wikipedia articles\n",
        "    num_documents -- total number of documents\n",
        "    \n",
        "    Returns:\n",
        "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
        "    list of document these keys appears in (and the positions) as values.\n",
        "    tf - normalized term frequency for each term in each document\n",
        "    df - number of documents each term appear in\n",
        "    idf - inverse document frequency of each term\n",
        "    \"\"\"\n",
        "\n",
        "    index = defaultdict(list)\n",
        "    tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
        "    df = defaultdict(int)  #document frequencies of terms in the corpus\n",
        "    title_index = defaultdict(str)\n",
        "    idf = defaultdict(float)\n",
        "\n",
        "    for line in lines:\n",
        "        line_arr = line.split(\"|\")\n",
        "        page_id = int(line_arr[0])\n",
        "        terms = build_terms(''.join(line_arr[1:]))  #page_title + page_text\n",
        "        title = line_arr[1]\n",
        "        title_index[page_id] = title\n",
        "\n",
        "        ## ===============================================================        \n",
        "        ## create the index for the **current page** and store it in current_page_index\n",
        "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
        "\n",
        "        ## Example: if the curr_doc has id 1 and his text is \n",
        "        ##\"web retrieval information retrieval\":\n",
        "\n",
        "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
        "\n",
        "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
        "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
        "        ## ===============================================================\n",
        "\n",
        "        current_page_index = {}\n",
        "\n",
        "        for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
        "            try:\n",
        "                # if the term is already in the dict append the position to the corresponding list\n",
        "                current_page_index[term][1].append(position) \n",
        "            except:\n",
        "                # Add the new term as dict key and initialize the array of positions and add the position\n",
        "                current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
        "\n",
        "        # normalize term frequencies\n",
        "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
        "        # norm is the same for all terms of a document.\n",
        "        norm = 0\n",
        "        for term, posting in current_page_index.items():\n",
        "            # posting will contain the list of positions for current term in current document. \n",
        "            # posting ==> [current_doc, [list of positions]] \n",
        "            # you can use it to infer the frequency of current term.\n",
        "            norm += len(posting[1]) ** 2\n",
        "        norm = math.sqrt(norm)\n",
        "\n",
        "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
        "        for term, posting in current_page_index.items():\n",
        "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
        "            tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
        "            #increment the document frequency of current term (number of documents containing the current term)\n",
        "            df[term] += 1 # increment DF for current term\n",
        "\n",
        "        #merge the current page index with the main index\n",
        "        for term_page, posting_page in current_page_index.items():\n",
        "            index[term_page].append(posting_page)\n",
        "\n",
        "        # Compute IDF following the formula (3) above. HINT: use np.log\n",
        "        for term in df:\n",
        "            idf[term] = np.round(np.log(float(num_documents / df[term])), 4)\n",
        "\n",
        "    return index, tf, df, idf, title_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNEF5Ezg5eXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e290f8-af9a-4e98-e831-2bbe1fc0cab2"
      },
      "source": [
        "start_time = time.time()\n",
        "num_documents = len(lines)\n",
        "index, tf, df, idf, title_index = create_index_tfidf(lines, num_documents)\n",
        "print(\"Total time to create the TD-IDF index: {} seconds\" .format(np.round(time.time() - start_time, 2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time to create the TD-IDF index: 103.79 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NG6ooqn5eXj"
      },
      "source": [
        "\n",
        "def rank_documents(terms, docs, index, idf, tf, title_index):\n",
        "    \"\"\"\n",
        "    Perform the ranking of the results of a search based on the tf-idf weights\n",
        "    \n",
        "    Argument:\n",
        "    terms -- list of query terms\n",
        "    docs -- list of documents, to rank, matching the query\n",
        "    index -- inverted index data structure\n",
        "    idf -- inverted document frequencies\n",
        "    tf -- term frequencies\n",
        "    title_index -- mapping between page id and page title\n",
        "    \n",
        "    Returns:\n",
        "    Print the list of ranked documents\n",
        "    \"\"\"\n",
        "\n",
        "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
        "    # The remaining elements would became 0 when multiplied to the query_vector\n",
        "    doc_vectors = defaultdict(lambda: [0] * len(terms)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
        "    query_vector = [0] * len(terms)\n",
        "\n",
        "    # compute the norm for the query tf\n",
        "    query_terms_count = collections.Counter(terms)  # get the frequency of each term in the query. \n",
        "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
        "    #HINT: use when computing tf for query_vector\n",
        "\n",
        "    query_norm = la.norm(list(query_terms_count.values()))\n",
        "\n",
        "    for termIndex, term in enumerate(terms):  #termIndex is the index of the term in the query\n",
        "        if term not in index:\n",
        "            continue\n",
        "\n",
        "        # TODO: check how to vectorize the query\n",
        "        # query_vector[termIndex]=idf[term]  # original\n",
        "        ## Compute tf*idf(normalize TF as done with documents)\n",
        "        query_vector[termIndex] = query_terms_count[term] / query_norm * idf[term]\n",
        "\n",
        "        # Generate doc_vectors for matching docs\n",
        "        for doc_index, (doc, postings) in enumerate(index[term]):\n",
        "            # Example of [doc_index, (doc, postings)]\n",
        "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
        "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
        "            # term is in doc 26 in positions 1,4, .....\n",
        "            # term is in doc 33 in positions 26,33, .....\n",
        "\n",
        "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
        "            if doc in docs:\n",
        "                doc_vectors[doc][termIndex] = tf[term][doc_index] * idf[term]  # TODO: check if multiply for idf\n",
        "\n",
        "    # Calculate the score of each doc \n",
        "    # compute the cosine similarity between queyVector and each docVector:\n",
        "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
        "    # see np.dot\n",
        "    \n",
        "    doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
        "    doc_scores.sort(reverse=True)\n",
        "    result_docs = [x[1] for x in doc_scores]\n",
        "    #print document titles instead if document id's\n",
        "    #result_docs=[ title_index[x] for x in result_docs ]\n",
        "    if len(result_docs) == 0:\n",
        "        print(\"No results found, try again\")\n",
        "        query = input()\n",
        "        docs = search_tf_idf(query, index)\n",
        "    #print ('\\n'.join(result_docs), '\\n')\n",
        "    return result_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbdnvSZf5eXk"
      },
      "source": [
        "def search_tf_idf(query, index):\n",
        "    \"\"\"\n",
        "    output is the list of documents that contain any of the query terms. \n",
        "    So, we will get the list of documents for each query term, and take the union of them.\n",
        "    \"\"\"\n",
        "    query = build_terms(query)\n",
        "    docs = set()\n",
        "    for term in query:\n",
        "        try:\n",
        "            # store in term_docs the ids of the docs that contain \"term\"                        \n",
        "            term_docs = [posting[0] for posting in index[term]]\n",
        "            \n",
        "            # docs = docs Union term_docs\n",
        "            docs |= set(term_docs)\n",
        "        except:\n",
        "            #term is not in index\n",
        "            pass\n",
        "    docs = list(docs)\n",
        "    ranked_docs = rank_documents(query, docs, index, idf, tf, title_index)\n",
        "    return ranked_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD-2cPRG5eXk"
      },
      "source": [
        "### Results with ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhnq3x-j5eXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2b8ae5-0868-4c66-fba6-8e8d9fb77d66"
      },
      "source": [
        "print(\"Insert your query (i.e.: Computer Science):\\n\")\n",
        "query = input()\n",
        "ranked_docs = search_tf_idf(query, index)\n",
        "top = 10\n",
        "\n",
        "print(\"\\n======================\\nTop {} results out of {} for the searched query:\\n\".format(top, len(ranked_docs)))\n",
        "for d_id in ranked_docs[:top]:\n",
        "    print(\"page_id= {} - page_title: {}\".format(d_id, title_index[d_id]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert your query (i.e.: Computer Science):\n",
            "\n",
            "Computer Science\n",
            "\n",
            "======================\n",
            "Top 10 results out of 345 for the searched query:\n",
            "\n",
            "page_id= 2583 - page_title: Association of Synchronous Data Formats\n",
            "page_id= 297 - page_title: ACM Portal\n",
            "page_id= 2086 - page_title: Aperiodic finite state automaton\n",
            "page_id= 2784 - page_title: Australian Partnership for Advanced Computing\n",
            "page_id= 1220 - page_title: Aggregate function\n",
            "page_id= 1702 - page_title: American flag sort\n",
            "page_id= 647 - page_title: A Sharp   NET \n",
            "page_id= 956 - page_title: Adaptive Behavior\n",
            "page_id= 3939 - page_title: Bipolar violation\n",
            "page_id= 3190 - page_title: BTSharp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE7_Rq4D5eXk"
      },
      "source": [
        "# The end"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}