{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "iiB2f3Y-5eXS",
   "metadata": {
    "id": "iiB2f3Y-5eXS"
   },
   "source": [
    "# Information Retrieval and Web Analytics\n",
    "\n",
    "# Part 1: Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf6d8963-aa98-40f2-8399-94b866d9c18e",
    "outputId": "bc13f731-861e-4279-d074-c9603a8eadce"
   },
   "outputs": [],
   "source": [
    "# mount google drive if using google collab, else skip\n",
    "# we are not using it because it is more comfortable to use jupyter lab\n",
    "\n",
    "BASEDIR = '.'\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASEDIR = 'drive/MyDrive'\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2a1daf4-842d-4c00-b294-0efef0747570",
    "outputId": "df3d0a85-9c68-4f9b-be0a-af9e5d5ebc82"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rafaelbardisarodes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# required imports for the notebook\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "HeK-m09ohD79",
   "metadata": {
    "id": "HeK-m09ohD79"
   },
   "outputs": [],
   "source": [
    "# read the json file as a dataframe\n",
    "df = pd.read_json(f'{BASEDIR}/data/tw_hurricane_data.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b_miHSTziZ78",
   "metadata": {
    "id": "b_miHSTziZ78"
   },
   "outputs": [],
   "source": [
    "# create a dataframe with the features wanted\n",
    "tweets = df[['id','full_text', 'user','created_at','entities', 'favorite_count', 'retweet_count']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "W5rX-Zv-vOVN",
   "metadata": {
    "id": "W5rX-Zv-vOVN"
   },
   "outputs": [],
   "source": [
    "# correct different features inside the dataframe\n",
    "# create lists for the features we want to modify\n",
    "url=[]\n",
    "hashtags=[]\n",
    "user=[]\n",
    "# iterate through the whole dataset\n",
    "for ele in range(len(tweets)):\n",
    "    hashtags.append([hashtag['text'] for hashtag in tweets['entities'][ele]['hashtags']])  # extract the hashtags\n",
    "    user.append(tweets['user'][ele]['name'])                    # extract user names\n",
    "    try:\n",
    "        url.append(tweets['entities'][ele]['media'][0][\"expanded_url\"]) # extract url if this exists\n",
    "    except: \n",
    "        url.append('')\n",
    "# assign this lists to columns to dataframe\n",
    "tweets['url'] = url\n",
    "tweets['hashtags'] = hashtags\n",
    "tweets['user'] = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "LPkyUQNJx-aB",
   "metadata": {
    "id": "LPkyUQNJx-aB"
   },
   "outputs": [],
   "source": [
    "# drop the column used before that would not be needed after\n",
    "tweets.drop(['entities'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a5a51cd-0180-4a56-adf4-772ed54cbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Removes the characters:\n",
    "    !\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789\n",
    "    from the text.\n",
    "    \"\"\"\n",
    "\n",
    "    chars_to_remove = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "\n",
    "    tr = str.maketrans(\"\", \"\", chars_to_remove)\n",
    "\n",
    "    return text.translate(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "RQLRhINXeYGV",
   "metadata": {
    "id": "RQLRhINXeYGV"
   },
   "outputs": [],
   "source": [
    "# reuse of the function shown in class to transform text into lowercase and erase stop words...\n",
    "def build_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "    \n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "    \n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    line = line.lower()\n",
    "    \n",
    "    # tremendo pero aligual rompe algo despues\n",
    "    line = remove_punctuation(line)\n",
    "    \n",
    "    line = line.split()  # Tokenize the text to get a list of terms\n",
    "    line = [x for x in line if x not in stop_words]  # eliminate the stopwords\n",
    "    line = [stemmer.stem(word) for word in line] # perform stemming (HINT: use List Comprehension)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hceA51tF5SK9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hceA51tF5SK9",
    "outputId": "9434e7cc-4d33-4ab2-9b81-c040705807e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/r5h7_tb55pvdx8zbc30wjdm80000gp/T/ipykernel_25852/3457202907.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['full_text'][ele] = ' '.join(text)\n",
      "/var/folders/98/r5h7_tb55pvdx8zbc30wjdm80000gp/T/ipykernel_25852/3457202907.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['user'][ele] = tweets['user'][ele].lower()\n",
      "/var/folders/98/r5h7_tb55pvdx8zbc30wjdm80000gp/T/ipykernel_25852/3457202907.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets['hashtags'][ele] = list(map(str.lower, tweets['hashtags'][ele]))\n"
     ]
    }
   ],
   "source": [
    "# use the function above to correct the tweet and also convert into lowercase the hastags and usernames\n",
    "for ele in range(len(tweets)):\n",
    "    text = build_terms(tweets['full_text'][ele])\n",
    "    text = [word for word in text if word.startswith('#')==False]\n",
    "    text = [word for word in text if word.startswith('@')==False]\n",
    "    text = [word for word in text if word.startswith('http')==False]\n",
    "    tweets['full_text'][ele] = ' '.join(text)\n",
    "    tweets['user'][ele] = tweets['user'][ele].lower()\n",
    "    # hashtags may be more than one, so apply lowercase function to all its elements\n",
    "    tweets['hashtags'][ele] = list(map(str.lower, tweets['hashtags'][ele]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28lVkxbeLE5g",
   "metadata": {
    "id": "28lVkxbeLE5g"
   },
   "outputs": [],
   "source": [
    "# get dictionary to map tweet ids to doc ids\n",
    "# we know the ids file is a list of [doc_id \\t tweet_id]\n",
    "with open(f'{BASEDIR}/data/tweet_document_ids_map.csv', 'r') as id_file:\n",
    "    ids = csv.reader(id_file, delimiter=\"\\t\")\n",
    "    dict_ids = {id_to_id[1]: id_to_id[0] for id_to_id in list(ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GMZdPWXcLjN4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMZdPWXcLjN4",
    "outputId": "0ad5e075-b8d6-4e98-fac2-8fbf86e665d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/98/r5h7_tb55pvdx8zbc30wjdm80000gp/T/ipykernel_25852/3321124730.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets[\"id\"][ele] = dict_ids[tweet_id]\n"
     ]
    }
   ],
   "source": [
    "# map tweet ids with doc ids\n",
    "for ele in range(len(tweets)):\n",
    "    tweet_id = str(tweets[\"id\"][ele])\n",
    "    tweets[\"id\"][ele] = dict_ids[tweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3Pjhk-3EUITY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "3Pjhk-3EUITY",
    "outputId": "19946bdd-ec3d-4f10-a967-411033d68f96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>url</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1</td>\n",
       "      <td>keep spin us pmâ€¦go away alreadi hurricaneian</td>\n",
       "      <td>suzðŸ‘»</td>\n",
       "      <td>2022-09-30 18:39:08+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/suzjdean/status/1575918182...</td>\n",
       "      <td>[hurricaneian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2</td>\n",
       "      <td>heart go affect hurricaneian wish everyon road...</td>\n",
       "      <td>lytx</td>\n",
       "      <td>2022-09-30 18:39:01+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[hurricaneian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_3</td>\n",
       "      <td>kissimme neighborhood michigan ave hurricaneian</td>\n",
       "      <td>christopher heath</td>\n",
       "      <td>2022-09-30 18:38:58+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/CHeathWFTV/status/15759181...</td>\n",
       "      <td>[hurricaneian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_4</td>\n",
       "      <td>one tree backyard scare poltergeist tree itâ€™ s...</td>\n",
       "      <td>alex âœ¨</td>\n",
       "      <td>2022-09-30 18:38:57+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[scwx, hurricaneian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_5</td>\n",
       "      <td>ashleyruizwx stephan lilmizzheidi mrsniffl win...</td>\n",
       "      <td>tess ðŸ’‹</td>\n",
       "      <td>2022-09-30 18:38:53+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[hurricaneian]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          full_text  \\\n",
       "0  doc_1       keep spin us pmâ€¦go away alreadi hurricaneian   \n",
       "1  doc_2  heart go affect hurricaneian wish everyon road...   \n",
       "2  doc_3    kissimme neighborhood michigan ave hurricaneian   \n",
       "3  doc_4  one tree backyard scare poltergeist tree itâ€™ s...   \n",
       "4  doc_5  ashleyruizwx stephan lilmizzheidi mrsniffl win...   \n",
       "\n",
       "                user                created_at  favorite_count  retweet_count  \\\n",
       "0               suzðŸ‘» 2022-09-30 18:39:08+00:00               0              0   \n",
       "1               lytx 2022-09-30 18:39:01+00:00               0              0   \n",
       "2  christopher heath 2022-09-30 18:38:58+00:00               0              0   \n",
       "3             alex âœ¨ 2022-09-30 18:38:57+00:00               0              0   \n",
       "4             tess ðŸ’‹ 2022-09-30 18:38:53+00:00               0              0   \n",
       "\n",
       "                                                 url              hashtags  \n",
       "0  https://twitter.com/suzjdean/status/1575918182...        [hurricaneian]  \n",
       "1                                                           [hurricaneian]  \n",
       "2  https://twitter.com/CHeathWFTV/status/15759181...        [hurricaneian]  \n",
       "3                                                     [scwx, hurricaneian]  \n",
       "4                                                           [hurricaneian]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the head of the dataframe to visualize our result\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bdc3a76-824d-4f5a-af97-41cc35ea1b8c",
   "metadata": {
    "id": "4bdc3a76-824d-4f5a-af97-41cc35ea1b8c"
   },
   "outputs": [],
   "source": [
    "# save result in a new csv file\n",
    "tweets.to_csv(f'{BASEDIR}/data/processed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29ae45a-ae44-46be-aba6-6bcd9be01393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Implement the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a Python dictionary) containing terms as keys and the corresponding\n",
    "    list of documents where these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index = defaultdict(list)\n",
    "    title_index = {}  # dictionary to map page titles to page ids\n",
    "\n",
    "    # ight esto no es correcto, tenemos tremenda lista de tweets, no documentos\n",
    "    for line in lines:  # Remember, lines contain all documents from file\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "\n",
    "        terms = build_terms(''.join(line_arr[1:]))  # page_title + page_text\n",
    "\n",
    "        title = line_arr[1]\n",
    "        title_index[page_id] = title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "        ## current_page_index ==> { â€˜term1â€™: [current_doc, [list of positions]], ...,â€˜term_nâ€™: [current_doc, [list of positions]]}\n",
    "\n",
    "        ## Example: if the curr_doc has id 1 and his text is \"web retrieval information retrieval\":\n",
    "\n",
    "        ## current_page_index ==> { â€˜webâ€™: [1, [0]], â€˜retrievalâ€™: [1, [1,4]], â€˜informationâ€™: [1, [2]]}\n",
    "\n",
    "        ## the term â€˜webâ€™ appears in document 1 in positions 0, \n",
    "        ## the term â€˜retrievalâ€™ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "\n",
    "        current_page_index = {}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (current_page_index)\n",
    "                # append the position to the corresponding list\n",
    "                current_page_index[term][1].append(position)\n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "        # merge the current page index with the main index\n",
    "        for term_page, posting_page in current_page_index.items():\n",
    "            index[term_page].append(posting_page)\n",
    "    return index, title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c491f28-1d3b-4eeb-be57-aeab3ed54777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48d6e1-4892-423a-89b4-1f88cf895b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
